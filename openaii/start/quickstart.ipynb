{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ä½¿ç”¨Sparkæ¨¡å‹å®è·µLangChainåŠŸèƒ½"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import logging\n",
    "\n",
    "# è®¾ç½®æ—¥å¿—çº§åˆ«\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = OpenAI()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ['è®²ä¸€ä¸ªå…³äºä¸­å›½è¶³çƒçš„ç¬‘è¯'], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E3490>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248991A84C0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E34C0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 08:44:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'1574'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-ratelimit-remaining-requests', b'198'), (b'x-ratelimit-remaining-tokens', b'149735'), (b'x-ratelimit-reset-requests', b'7m59.307s'), (b'x-ratelimit-reset-tokens', b'106ms'), (b'x-request-id', b'req_f2951314a5dda580b4f3463221d6019c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=C2HRL2Z6HGKFO9Z0qxxGi2Op1GwfE5dG8ujZT6WkmOk-1708332271-1.0-AY67XZk0CwBngYtgAFarQUhgO87V0fnVS2TAcoWaSjcLb7XBV4rTVhYta5NqopUeC2oZrPMceDUj8D18rIgtAJM=; path=/; expires=Mon, 19-Feb-24 09:14:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=pBGVzSpu077xwItxSuiMsBYueJPzuVScUHLIzFh81rQ-1708332271176-0.0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d3a6b4d318971-SIN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/completions \"200 OK\"\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'è®²ä¸€ä¸ªå…³äºä¸­å›½è¶³çƒçš„ç¬‘è¯'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E3BB0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248FFEBA240> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248FFF75060>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 08:44:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'890'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'109'), (b'x-ratelimit-remaining-tokens', b'39974'), (b'x-ratelimit-reset-requests', b'10h51m50.405s'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_75e88ecc930d3a2a005f3e2b0568e806'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d3a7b48c63fe0-SIN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='ä¸ºä»€ä¹ˆä¸­å›½è¶³çƒé˜Ÿæ€»æ˜¯è¾“çƒï¼Ÿ\\n\\nå› ä¸ºä»–ä»¬æ¯æ¬¡æ¯”èµ›éƒ½æŠŠçƒè¸¢åˆ°äº†é•¿åŸä¸Šï¼')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"è®²ä¸€ä¸ªå…³äºä¸­å›½è¶³çƒçš„ç¬‘è¯\")\n",
    "# llm.invoke(\"è®²ä¸€ä¸ªå…³äºä¸­å›½è¶³çƒçš„ç¬‘è¯\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '\\nè¿™æ˜¯ä¸€æœ¬å†å²ç©¿è¶Šå°è¯´çš„å¤§çº²ï¼Œå…·ä½“æƒ…èŠ‚å¦‚ä¸‹ï¼š\\n\\næ ‡é¢˜ï¼šã€Šä»£ç ç©¿è¶Šï¼šç¨‹åºå‘˜çš„å®‹æœä¹‹æ—…ã€‹\\n\\nç¬¬ä¸€ç« ï¼šæ„å¤–ç©¿è¶Š\\n\\nåœ¨ä¸€ä¸ªç¹å¿™çš„éƒ½å¸‚è¡—é“ä¸Šï¼Œå¹´è½»çš„ç¨‹åºå‘˜ææ˜æ­£åœ¨åŒ†å¿™èµ¶å¾€å…¬å¸ã€‚ä»–ä¸“æ³¨åœ°ç›¯ç€æ‰‹æœºå±å¹•ï¼Œå®Œå…¨æ²¡æœ‰æ³¨æ„åˆ°æ¥å¾€çš„è½¦è¾†ã€‚çªç„¶ï¼Œä¸€è¾†ç–¾é©°è€Œæ¥çš„å¡è½¦å†²å‘äº†ä»–â€¦â€¦\\n\\nææ˜é†’æ¥æ—¶ï¼Œå‘ç°è‡ªå·±èººåœ¨ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹ã€‚å‘¨å›´æ˜¯ä¸€ç‰‡å®é™çš„ç”°é‡ï¼Œè¿œå¤„æ˜¯é’å±±èµ·ä¼ã€‚ä»–æƒŠè®¶åœ°å‘ç°è‡ªå·±å·²ç»ä¸åœ¨ç°ä»£åŸå¸‚ï¼Œè€Œæ˜¯æ¥åˆ°äº†ä¸€ç‰‡å¤è€çš„åœŸåœ°ä¸Šã€‚\\n\\nç¬¬äºŒç« ï¼šå¤ä»£ç”Ÿæ´»\\n\\nææ˜æ„è¯†åˆ°è‡ªå·±ç©¿è¶Šåˆ°äº†å®‹æœã€‚ä»–è¢«ä¸€ä¸ªå–„è‰¯çš„å†œå¤«æ”¶ç•™ï¼Œæˆä¸ºäº†ä»–çš„å„¿å­ã€‚åœ¨è¿™ä¸ªé™Œç”Ÿçš„æ—¶ä»£ï¼Œä»–å­¦ä¼šäº†è€•ç§ã€å…»èš•ï¼Œä½“éªŒäº†å¤ä»£äººçš„æœ´ç´ ç”Ÿæ´»ã€‚\\n\\nç„¶è€Œï¼Œææ˜å¿ƒä¸­ä»ç„¶ç‡ƒçƒ§ç€å¯¹çŸ¥è¯†çš„æ¸´æœ›ã€‚ä»–åŠªåŠ›è‡ªå­¦ï¼Œå¤œä»¥ç»§æ—¥åœ°é’»ç ”ç»ä¹¦ã€‚ä»–æ˜ç™½ï¼Œåªæœ‰é€šè¿‡çŸ¥è¯†æ‰èƒ½æ”¹å˜è‡ªå·±çš„å‘½è¿ã€‚\\n\\nç¬¬ä¸‰ç« ï¼šæ±‚å­¦ä¹‹è·¯\\n\\nææ˜å†³å®šæ”¹å˜è‡ªå·±çš„å‘½è¿ã€‚ä»–å››å¤„å¯»æ‰¾æœºä¼šï¼Œç»ˆäºå¾—åˆ°äº†ä¸€ä½ä¹¦é¦™é—¨ç¬¬çš„å­å¼Ÿçš„æŒ‡ç‚¹ï¼Œå¼€å§‹æ­£å¼æ±‚å­¦ã€‚\\n\\nåœ¨ä¸€ä½éšå±…å±±æ—çš„è€å„’å®¶é—¨ä¸‹ï¼Œææ˜æ¥å—äº†è‰°è‹¦çš„ä¿®è¡Œã€‚ä»–å­¦ä¹ ç»å²å­é›†ï¼Œæ¢è®¨å“²ç†ï¼Œæ±²å–å¤äººæ™ºæ…§ã€‚\\n\\nææ˜å¹¶ä¸æ»¡è¶³äºåªæ˜¯æ»¡è¶³äºç°çŠ¶ã€‚ä»–ç«‹å¿—è¦é€šè¿‡ç§‘ä¸¾è€ƒè¯•ï¼Œæˆä¸ºä¸€åè‘—åçš„å®˜å‘˜ï¼Œæ”¹å˜è¿™ä¸ªæ—¶ä»£çš„å‘½è¿ã€‚\\n\\nç¬¬å››ç« ï¼šç§‘ä¸¾ä¹‹è·¯\\n\\nææ˜å‡­å€Ÿç€åšéŸ§çš„æ„å¿—å’Œæ‰å®çš„å­¦è¯†ï¼Œç»ˆäºè€ƒä¸Šäº†è¿›å£«ã€‚ä»–çš„åå­—åœ¨ç§‘ä¸¾æ¦œä¸Šç† ç† ç”Ÿè¾‰ï¼Œæˆä¸ºäº†ä¼—äººç©ç›®çš„ç„¦ç‚¹ã€‚\\n\\nç„¶è€Œï¼Œè¿›å£«å¹¶ä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯æ–°çš„èµ·ç‚¹ã€‚ææ˜å†ç»è‰°éš¾é™©é˜»ï¼Œä¸€æ­¥æ­¥æ™‹å‡ï¼Œæœ€ç»ˆæˆä¸ºäº†ä¸€ä½å£°åæ˜¾èµ«çš„å®˜å‘˜ã€‚\\n\\nä»–ç”¨è‡ªå·±çš„æ‰åå’Œæ™ºæ…§ï¼Œæ¨åŠ¨ç€ç¤¾ä¼šçš„è¿›æ­¥ï¼Œæ”¹å–„ç€äººæ°‘çš„ç”Ÿæ´»ã€‚ä»–æˆä¸ºäº†ä¸€ä¸ªä¼ å¥‡ï¼Œä¸€ä¸ªç©¿è¶Šæ—¶ç©ºçš„ç¨‹åºå‘˜ï¼Œåœ¨å¤ä»£çš„ä¸–ç•Œä¹¦å†™ç€å±äºè‡ªå·±çš„ä¼ å¥‡ã€‚\\n\\nç¬¬äº”ç« ï¼šå›å½’ç°ä»£\\n\\nç»è¿‡å²æœˆçš„æ´—ç¤¼ï¼Œææ˜ç»ˆäºåœ¨å¤ä»£å»ºç«‹äº†è‡ªå·±çš„åŠŸä¸šã€‚ç„¶è€Œï¼Œä»–å¯¹ç°ä»£ç¤¾ä¼šçš„æ€å¿µå´æ„ˆå‘å¼ºçƒˆã€‚\\n\\nåœ¨ä¸€æ¬¡å¶ç„¶çš„æœºä¼šä¸‹ï¼Œææ˜å¾—çŸ¥äº†è¿”å›ç°ä»£çš„æ–¹æ³•ã€‚ä»–æ¯…ç„¶å†³ç„¶åœ°è¸ä¸Šäº†å›å½’çš„æ—…ç¨‹ã€‚\\n\\nå½“ä»–å†æ¬¡å›åˆ°äº†ç°ä»£çš„éƒ½å¸‚ï¼Œä¸€åˆ‡éƒ½å·²ç»ä¸åŒäº†ã€‚ç„¶è€Œï¼Œä»–å¸¦ç€å¤ä»£çš„æ™ºæ…§å’Œèƒ¸æ€€ï¼Œç»§ç»­ç€è‡ªå·±çš„ç”Ÿæ´»ï¼Œä¸ºç°ä»£ç¤¾ä¼šçš„è¿›æ­¥è´¡çŒ®ç€è‡ªå·±çš„åŠ›é‡ã€‚\\n\\nç»“å±€ï¼šææ˜è™½ç„¶å›åˆ°äº†ç°ä»£ï¼Œä½†ä»–å¿ƒä¸­å§‹ç»ˆæ€€æ£ç€å¯¹å¤ä»£çš„å›å¿†å’Œæ•¬æ„ã€‚ä»–ç”¨è‡ªå·±çš„åŒæ‰‹ä¹¦å†™ç€è·¨è¶Šæ—¶ç©ºçš„ä¼ å¥‡ï¼Œæˆä¸ºäº†ä¸€ä¸ªæ°¸æ’çš„ä¼ å¥‡äººç‰©ã€‚\\n'}, {'role': 'user', 'content': 'æ ¹æ®å°è¯´å¤§çº²å°†ç¬¬äº”ç« æ‰©å±•ä¸º5000å­—å·¦å³è¯¦ç»†æƒ…èŠ‚'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': True, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 20 Feb 2024 07:17:11 GMT'), (b'Content-Type', b'text/event-stream'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'269'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'195'), (b'x-ratelimit-remaining-tokens', b'39321'), (b'x-ratelimit-reset-requests', b'34m10.528s'), (b'x-ratelimit-reset-tokens', b'1.018s'), (b'x-request-id', b'req_6857a8c3028992493c76768f20ea920c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8584f7e7ac433f49-SIN'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬äº”ç« ï¼šå›å½’ç°ä»£\n",
      "\n",
      "ææ˜ç©¿è¶Šå›ç°ä»£åï¼Œä»–çš„å†…å¿ƒå……æ»¡äº†å¤æ‚çš„æƒ…ç»ªã€‚ä¸€æ–¹é¢ï¼Œä»–æ€€å¿µåœ¨å¤ä»£çš„é‚£æ®µå†ç¨‹ï¼Œæ€€å¿µåœ¨å®‹æœçš„æœ´ç´ ç”Ÿæ´»å’Œå¯¹çŸ¥è¯†çš„è¿½æ±‚ï¼›å¦ä¸€æ–¹é¢ï¼Œä»–ä¹Ÿæ„è¯†åˆ°ç°ä»£ç¤¾ä¼šçš„å‘å±•å’Œå˜åŒ–ï¼Œæ„Ÿå—åˆ°äº†ç§‘æŠ€å¸¦æ¥çš„ä¾¿åˆ©å’Œè¿›æ­¥ã€‚ç„¶è€Œï¼Œä»–æ·±çŸ¥è‡ªå·±çš„è´£ä»»æ˜¯åœ¨å½“ä¸‹çš„ç°ä»£ç¤¾ä¼šä¸ºä¹‹è´¡çŒ®è‡ªå·±çš„æ™ºæ…§å’ŒåŠ›é‡ã€‚\n",
      "\n",
      "1.é€‚åº”ç°ä»£ç”Ÿæ´»\n",
      "\n",
      "å›åˆ°ç°ä»£åï¼Œææ˜å‘ç°è‡ªå·±å¯¹äºç°ä»£çš„ä¸€åˆ‡éƒ½æœ‰ä¸€ç§æ–°çš„è§†è§’ã€‚ä»–é‡æ–°é€‚åº”ç€ç°ä»£çš„ç”Ÿæ´»æ–¹å¼ï¼Œé‡æ–°ç†Ÿæ‚‰ç€é«˜æ¥¼å¤§å¦ã€ç¹åè¡—é“å’Œå¿«èŠ‚å¥çš„ç”Ÿæ´»èŠ‚å¥ã€‚\n",
      "\n",
      "ä»–ç§Ÿä¸‹ä¸€é—´å°å…¬å¯“ï¼Œå¼€å§‹äº†è‡ªå·±çš„æ–°ç”Ÿæ´»ã€‚å°½ç®¡ç°ä»£çš„ç§‘æŠ€å’Œç”Ÿæ´»æ–¹å¼ä¸å¤ä»£è¿¥ç„¶ä¸åŒï¼Œä½†ä»–ä¾ç„¶ä¿æŒç€å¯¹çŸ¥è¯†çš„è¿½æ±‚å’Œå¯¹äººæ–‡æƒ…æ€€çš„çƒ­çˆ±ã€‚\n",
      "\n",
      "2.åº”ç”¨å¤ä»£æ™ºæ…§\n",
      "\n",
      "åœ¨ç°ä»£ç¤¾ä¼šï¼Œææ˜å‘ç°è‡ªå·±å¯¹å¤ä»£çš„å­¦è¯†å’Œæ™ºæ…§æœ‰ç€ç‹¬ç‰¹çš„åº”ç”¨ä¹‹å¤„ã€‚ä»–å°†å¤ä»£çš„å“²å­¦æ€æƒ³å’Œç®¡ç†ç»éªŒè¿ç”¨åˆ°ç°ä»£çš„å·¥ä½œä¸­ï¼Œå–å¾—äº†æ„æƒ³ä¸åˆ°çš„æ•ˆæœã€‚\n",
      "\n",
      "ä»–ä»¥å¤ä»£çš„â€œä»ä¹‰ç¤¼æ™ºä¿¡â€ä¸ºå‡†åˆ™ï¼Œå¤„ç†å·¥ä½œä¸­çš„å„ç§å¤æ‚å…³ç³»ï¼›ä»–ä»¥å¤ä»£çš„â€œå¿ å­èŠ‚ä¹‰â€ä¸ºä¿¡æ¡ï¼Œå¯¹å¾…èº«è¾¹çš„åŒäº‹å’Œæœ‹å‹ã€‚è¿™äº›å¤ä»£æ™ºæ…§æˆä¸ºäº†ä»–åœ¨ç°ä»£ç¤¾ä¼šç«‹è¶³çš„é‡è¦æ³•å®ã€‚\n",
      "\n",
      "3.å½±å“ç°ä»£ç¤¾ä¼š\n",
      "\n",
      "éšç€æ—¶é—´çš„æ¨ç§»ï¼Œææ˜é€æ¸åœ¨ç°ä»£ç¤¾ä¼šä¸­å´­éœ²å¤´è§’ã€‚ä»–å‡­å€Ÿç€æ‰å®çš„å­¦è¯†å’Œå“è¶Šçš„èƒ½åŠ›ï¼Œæˆä¸ºäº†å…¬å¸ä¸­ä¸å¯æˆ–ç¼ºçš„äººæ‰ã€‚\n",
      "\n",
      "ä»–åœ¨å·¥ä½œä¸­æå‡ºäº†è®¸å¤šæ–°é¢–çš„æƒ³æ³•å’Œè§£å†³æ–¹æ¡ˆï¼Œå—åˆ°äº†é¢†å¯¼å’ŒåŒäº‹çš„é«˜åº¦èµæ‰¬ã€‚ä»–ç”¨å¤ä»£çš„æ™ºæ…§å’Œç°ä»£çš„ç§‘æŠ€ç›¸ç»“åˆï¼Œä¸ºå…¬å¸å¸¦æ¥äº†æ–°çš„å‘å±•æœºé‡ã€‚\n",
      "\n",
      "4.å¯»æ‰¾å¤ä»£çº¿ç´¢\n",
      "\n",
      "ç„¶è€Œï¼Œææ˜çš„å†…å¿ƒå§‹ç»ˆå­˜åœ¨ç€ä¸€ç§å¯¹å¤ä»£çš„ç•™æ‹å’Œæ€å¿µã€‚åœ¨ä¸šä½™æ—¶é—´ï¼Œä»–å¼€å§‹å¯»æ‰¾å¤ä»£çº¿ç´¢ï¼Œæ¢ç©¶å¤ä»£æ–‡æ˜çš„å¥¥ç§˜å’Œæ™ºæ…§ã€‚\n",
      "\n",
      "ä»–ç ”ç©¶å¤ä»£æ–‡çŒ®ï¼Œæ¢è®¨å¤ä»£ç§‘æŠ€çš„å‘å±•ï¼Œå¯»æ‰¾å¤ä»£ç•™ä¸‹çš„ç§ç§çº¿ç´¢ã€‚ä»–å¸Œæœ›èƒ½å¤Ÿå°†å¤ä»£çš„æ™ºæ…§å’Œç°ä»£çš„ç§‘æŠ€èåˆèµ·æ¥ï¼Œä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥æ›´å¤§çš„è¿›æ­¥å’Œå‘å±•ã€‚\n",
      "\n",
      "5.è·¨è¶Šæ—¶ç©ºçš„ä¼ å¥‡\n",
      "\n",
      "ææ˜æˆä¸ºäº†ä¸€ååœ¨ç°ä»£ç¤¾ä¼šå¤‡å—å°Šæ•¬çš„å­¦è€…å’Œä¸“å®¶ã€‚ä»–ä¸ä»…åœ¨å·¥ä½œä¸­å–å¾—äº†å“è¶Šæˆå°±ï¼Œè¿˜ç§¯æå‚ä¸å…¬ç›Šäº‹ä¸šï¼Œä¸ºç¤¾ä¼šè´¡çŒ®ç€è‡ªå·±çš„åŠ›é‡ã€‚\n",
      "\n",
      "ä»–ç”¨è‡ªå·±çš„æ™ºæ…§å’Œèƒ¸æ€€ï¼Œä¹¦å†™ç€è·¨è¶Šæ—¶ç©ºçš„ä¼ å¥‡ã€‚ä»–æˆä¸ºäº†ä¸€ä¸ªè¿æ¥å¤ä»£ä¸ç°ä»£çš„æ¡¥æ¢ï¼Œå°†å¤ä»£çš„æ™ºæ…§å’Œç°ä»£çš„ç§‘æŠ€ç´§å¯†ç›¸è¿ï¼Œä¸ºäººç±»ç¤¾ä¼šçš„å‘å±•å¼€è¾Ÿå‡ºæ–°çš„é“è·¯ã€‚\n",
      "\n",
      "ç»“å±€\n",
      "\n",
      "ææ˜è™½ç„¶èº«å¤„ç°ä»£ç¤¾ä¼šï¼Œä½†ä»–çš„å†…å¿ƒå§‹ç»ˆæ€€æ£ç€å¯¹å¤ä»£çš„çƒ­çˆ±å’Œæ•¬æ„ã€‚ä»–ç”¨è‡ªå·±çš„æ™ºæ…§å’ŒåŠªåŠ›ï¼Œä¸ºç°ä»£ç¤¾ä¼šçš„è¿›æ­¥å’Œå‘å±•è´¡çŒ®ç€è‡ªå·±çš„åŠ›é‡ï¼Œæˆä¸ºäº†ä¸€ä¸ªæ°¸æ’çš„ä¼ å¥‡äººç‰©ï¼Œç•™ä¸‹äº†ä¸æœ½çš„ä¼ è¯´ã€‚ä»–çš„æ•…äº‹æ¿€åŠ±ç€åäººï¼Œè®©äººä»¬æ˜ç™½ï¼Œæ— è®ºèº«å¤„ä½•æ—¶ä½•åœ°ï¼Œåªè¦æ€€æ£ç€å¯¹çŸ¥è¯†å’Œæ™ºæ…§çš„è¿½æ±‚ï¼Œå°±èƒ½åˆ›é€ å‡ºå±äºè‡ªå·±çš„ä¼ å¥‡ã€‚"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_template = '''\n",
    "è¿™æ˜¯ä¸€æœ¬å†å²ç©¿è¶Šå°è¯´çš„å¤§çº²ï¼Œå…·ä½“æƒ…èŠ‚å¦‚ä¸‹ï¼š\n",
    "\n",
    "æ ‡é¢˜ï¼šã€Šä»£ç ç©¿è¶Šï¼šç¨‹åºå‘˜çš„å®‹æœä¹‹æ—…ã€‹\n",
    "\n",
    "ç¬¬ä¸€ç« ï¼šæ„å¤–ç©¿è¶Š\n",
    "\n",
    "åœ¨ä¸€ä¸ªç¹å¿™çš„éƒ½å¸‚è¡—é“ä¸Šï¼Œå¹´è½»çš„ç¨‹åºå‘˜ææ˜æ­£åœ¨åŒ†å¿™èµ¶å¾€å…¬å¸ã€‚ä»–ä¸“æ³¨åœ°ç›¯ç€æ‰‹æœºå±å¹•ï¼Œå®Œå…¨æ²¡æœ‰æ³¨æ„åˆ°æ¥å¾€çš„è½¦è¾†ã€‚çªç„¶ï¼Œä¸€è¾†ç–¾é©°è€Œæ¥çš„å¡è½¦å†²å‘äº†ä»–â€¦â€¦\n",
    "\n",
    "ææ˜é†’æ¥æ—¶ï¼Œå‘ç°è‡ªå·±èººåœ¨ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹ã€‚å‘¨å›´æ˜¯ä¸€ç‰‡å®é™çš„ç”°é‡ï¼Œè¿œå¤„æ˜¯é’å±±èµ·ä¼ã€‚ä»–æƒŠè®¶åœ°å‘ç°è‡ªå·±å·²ç»ä¸åœ¨ç°ä»£åŸå¸‚ï¼Œè€Œæ˜¯æ¥åˆ°äº†ä¸€ç‰‡å¤è€çš„åœŸåœ°ä¸Šã€‚\n",
    "\n",
    "ç¬¬äºŒç« ï¼šå¤ä»£ç”Ÿæ´»\n",
    "\n",
    "ææ˜æ„è¯†åˆ°è‡ªå·±ç©¿è¶Šåˆ°äº†å®‹æœã€‚ä»–è¢«ä¸€ä¸ªå–„è‰¯çš„å†œå¤«æ”¶ç•™ï¼Œæˆä¸ºäº†ä»–çš„å„¿å­ã€‚åœ¨è¿™ä¸ªé™Œç”Ÿçš„æ—¶ä»£ï¼Œä»–å­¦ä¼šäº†è€•ç§ã€å…»èš•ï¼Œä½“éªŒäº†å¤ä»£äººçš„æœ´ç´ ç”Ÿæ´»ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œææ˜å¿ƒä¸­ä»ç„¶ç‡ƒçƒ§ç€å¯¹çŸ¥è¯†çš„æ¸´æœ›ã€‚ä»–åŠªåŠ›è‡ªå­¦ï¼Œå¤œä»¥ç»§æ—¥åœ°é’»ç ”ç»ä¹¦ã€‚ä»–æ˜ç™½ï¼Œåªæœ‰é€šè¿‡çŸ¥è¯†æ‰èƒ½æ”¹å˜è‡ªå·±çš„å‘½è¿ã€‚\n",
    "\n",
    "ç¬¬ä¸‰ç« ï¼šæ±‚å­¦ä¹‹è·¯\n",
    "\n",
    "ææ˜å†³å®šæ”¹å˜è‡ªå·±çš„å‘½è¿ã€‚ä»–å››å¤„å¯»æ‰¾æœºä¼šï¼Œç»ˆäºå¾—åˆ°äº†ä¸€ä½ä¹¦é¦™é—¨ç¬¬çš„å­å¼Ÿçš„æŒ‡ç‚¹ï¼Œå¼€å§‹æ­£å¼æ±‚å­¦ã€‚\n",
    "\n",
    "åœ¨ä¸€ä½éšå±…å±±æ—çš„è€å„’å®¶é—¨ä¸‹ï¼Œææ˜æ¥å—äº†è‰°è‹¦çš„ä¿®è¡Œã€‚ä»–å­¦ä¹ ç»å²å­é›†ï¼Œæ¢è®¨å“²ç†ï¼Œæ±²å–å¤äººæ™ºæ…§ã€‚\n",
    "\n",
    "ææ˜å¹¶ä¸æ»¡è¶³äºåªæ˜¯æ»¡è¶³äºç°çŠ¶ã€‚ä»–ç«‹å¿—è¦é€šè¿‡ç§‘ä¸¾è€ƒè¯•ï¼Œæˆä¸ºä¸€åè‘—åçš„å®˜å‘˜ï¼Œæ”¹å˜è¿™ä¸ªæ—¶ä»£çš„å‘½è¿ã€‚\n",
    "\n",
    "ç¬¬å››ç« ï¼šç§‘ä¸¾ä¹‹è·¯\n",
    "\n",
    "ææ˜å‡­å€Ÿç€åšéŸ§çš„æ„å¿—å’Œæ‰å®çš„å­¦è¯†ï¼Œç»ˆäºè€ƒä¸Šäº†è¿›å£«ã€‚ä»–çš„åå­—åœ¨ç§‘ä¸¾æ¦œä¸Šç† ç† ç”Ÿè¾‰ï¼Œæˆä¸ºäº†ä¼—äººç©ç›®çš„ç„¦ç‚¹ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œè¿›å£«å¹¶ä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯æ–°çš„èµ·ç‚¹ã€‚ææ˜å†ç»è‰°éš¾é™©é˜»ï¼Œä¸€æ­¥æ­¥æ™‹å‡ï¼Œæœ€ç»ˆæˆä¸ºäº†ä¸€ä½å£°åæ˜¾èµ«çš„å®˜å‘˜ã€‚\n",
    "\n",
    "ä»–ç”¨è‡ªå·±çš„æ‰åå’Œæ™ºæ…§ï¼Œæ¨åŠ¨ç€ç¤¾ä¼šçš„è¿›æ­¥ï¼Œæ”¹å–„ç€äººæ°‘çš„ç”Ÿæ´»ã€‚ä»–æˆä¸ºäº†ä¸€ä¸ªä¼ å¥‡ï¼Œä¸€ä¸ªç©¿è¶Šæ—¶ç©ºçš„ç¨‹åºå‘˜ï¼Œåœ¨å¤ä»£çš„ä¸–ç•Œä¹¦å†™ç€å±äºè‡ªå·±çš„ä¼ å¥‡ã€‚\n",
    "\n",
    "ç¬¬äº”ç« ï¼šå›å½’ç°ä»£\n",
    "\n",
    "ç»è¿‡å²æœˆçš„æ´—ç¤¼ï¼Œææ˜ç»ˆäºåœ¨å¤ä»£å»ºç«‹äº†è‡ªå·±çš„åŠŸä¸šã€‚ç„¶è€Œï¼Œä»–å¯¹ç°ä»£ç¤¾ä¼šçš„æ€å¿µå´æ„ˆå‘å¼ºçƒˆã€‚\n",
    "\n",
    "åœ¨ä¸€æ¬¡å¶ç„¶çš„æœºä¼šä¸‹ï¼Œææ˜å¾—çŸ¥äº†è¿”å›ç°ä»£çš„æ–¹æ³•ã€‚ä»–æ¯…ç„¶å†³ç„¶åœ°è¸ä¸Šäº†å›å½’çš„æ—…ç¨‹ã€‚\n",
    "\n",
    "å½“ä»–å†æ¬¡å›åˆ°äº†ç°ä»£çš„éƒ½å¸‚ï¼Œä¸€åˆ‡éƒ½å·²ç»ä¸åŒäº†ã€‚ç„¶è€Œï¼Œä»–å¸¦ç€å¤ä»£çš„æ™ºæ…§å’Œèƒ¸æ€€ï¼Œç»§ç»­ç€è‡ªå·±çš„ç”Ÿæ´»ï¼Œä¸ºç°ä»£ç¤¾ä¼šçš„è¿›æ­¥è´¡çŒ®ç€è‡ªå·±çš„åŠ›é‡ã€‚\n",
    "\n",
    "ç»“å±€ï¼šææ˜è™½ç„¶å›åˆ°äº†ç°ä»£ï¼Œä½†ä»–å¿ƒä¸­å§‹ç»ˆæ€€æ£ç€å¯¹å¤ä»£çš„å›å¿†å’Œæ•¬æ„ã€‚ä»–ç”¨è‡ªå·±çš„åŒæ‰‹ä¹¦å†™ç€è·¨è¶Šæ—¶ç©ºçš„ä¼ å¥‡ï¼Œæˆä¸ºäº†ä¸€ä¸ªæ°¸æ’çš„ä¼ å¥‡äººç‰©ã€‚\n",
    "'''\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# ç”¨æˆ·èŠå¤©æ¨¡æ¿å°±æ˜¯ç®€å•çš„ç”¨æˆ·èŠå¤©å†…å®¹\n",
    "human_template=\"{input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = chat_prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"input\": \"æ ¹æ®å°è¯´å¤§çº²å°†ç¬¬äº”ç« æ‰©å±•ä¸º5000å­—å·¦å³è¯¦ç»†æƒ…èŠ‚\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'ç»™æˆ‘è®²ä¸€ä¸ªå…³äºä¸­å›½è¶³çƒçš„ç¬‘è¯'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E1870>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248FFEBA240> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002489B4D96F0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 08:59:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'843'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'110'), (b'x-ratelimit-remaining-tokens', b'39972'), (b'x-ratelimit-reset-requests', b'10h43m43.662s'), (b'x-ratelimit-reset-tokens', b'42ms'), (b'x-request-id', b'req_66c9276665fdbc8a7819a9f3f8d196d6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.4gUkZOlkRJNknKuh0LngVZFaI_p6LCfIkhuc1IP5OM-1708333191-1.0-AflZ1WqyjwrK+JIVTOwwPOU/KTC68Ik0iIqj5pB7iZ4iCxjfa9b3MCtnJURBP0CH7FnUAzXVqBvwJvimIc0MA0E=; path=/; expires=Mon, 19-Feb-24 09:29:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d50e969105f5e-SIN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='ä¸ºä»€ä¹ˆä¸­å›½è¶³çƒé˜Ÿæ€»æ˜¯è¾“çƒï¼Ÿ\\nå› ä¸ºä»–ä»¬æ€»æ˜¯åœ¨è¸¢â€œä¸­å›½é˜Ÿâ€ã€‚')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"joke\"],\n",
    "    template=\"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{joke}çš„ç¬‘è¯\",\n",
    ")\n",
    "prompt = prompt_template.format(joke='ä¸­å›½è¶³çƒ')\n",
    "llm.invoke(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'ç»™æˆ‘è®²ä¸€ä¸ªå…³äºä¸­å›½è¶³çƒçš„ç¬‘è¯'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248990F6FE0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248FFEBA240> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002489B46BAF0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 09:06:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'981'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'110'), (b'x-ratelimit-remaining-tokens', b'39972'), (b'x-ratelimit-reset-requests', b'10h44m47.258s'), (b'x-ratelimit-reset-tokens', b'42ms'), (b'x-request-id', b'req_07df55581ec2d164ea3d7b5c6bb08ae9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=g0H1oWHN8jQ0tWgaVBar_stuxYUM8X4IhP2TqxSXrNw-1708333560-1.0-AZUSt9jod0/WIhj9d3ZLvACqkRg8SzUd1XoAG54ybfJHtYpMP/UZHV52jtQEJFAk14gYulcLwzfthCeO3lls2G0=; path=/; expires=Mon, 19-Feb-24 09:36:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=SEV__n2eLSnW5TLJXA6QmwMzDcaG5EJ2qdM4VMEscDQ-1708333560425-0.0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d59e87b3309f3-LAS'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'joke': 'ä¸­å›½è¶³çƒ', 'text': 'ä¸ºä»€ä¹ˆä¸­å›½è¶³çƒé˜Ÿæ€»æ˜¯è¾“çƒï¼Ÿ\\n\\nå› ä¸ºä»–ä»¬è€æ˜¯åœ¨æ‰¾â€œçƒâ€ä¸ç€ï¼ğŸ˜‚ğŸ˜‚ğŸ˜‚'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"ä¸­å›½è¶³çƒ\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PydanticOutputParser' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 19\u001B[0m\n\u001B[0;32m     15\u001B[0m new_parser \u001B[38;5;241m=\u001B[39m OutputFixingParser\u001B[38;5;241m.\u001B[39mfrom_llm(parser\u001B[38;5;241m=\u001B[39mparser, llm\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m     17\u001B[0m misformatted \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTom Hanks\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilm_names\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: [\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mForrest Gump\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m]}\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnew_parser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmisformatted\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'PydanticOutputParser' object is not callable"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "\n",
    "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n",
    "\n",
    "new_parser.parser(misformatted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}