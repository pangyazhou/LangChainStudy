{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 使用Spark模型实践LangChain功能"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='D:\\\\dev\\\\miniconda3\\\\envs\\\\langchain\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import logging\n",
    "\n",
    "# 设置日志级别\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = OpenAI()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ['讲一个关于中国足球的笑话'], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E3490>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248991A84C0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E34C0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 08:44:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'1574'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-ratelimit-remaining-requests', b'198'), (b'x-ratelimit-remaining-tokens', b'149735'), (b'x-ratelimit-reset-requests', b'7m59.307s'), (b'x-ratelimit-reset-tokens', b'106ms'), (b'x-request-id', b'req_f2951314a5dda580b4f3463221d6019c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=C2HRL2Z6HGKFO9Z0qxxGi2Op1GwfE5dG8ujZT6WkmOk-1708332271-1.0-AY67XZk0CwBngYtgAFarQUhgO87V0fnVS2TAcoWaSjcLb7XBV4rTVhYta5NqopUeC2oZrPMceDUj8D18rIgtAJM=; path=/; expires=Mon, 19-Feb-24 09:14:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=pBGVzSpu077xwItxSuiMsBYueJPzuVScUHLIzFh81rQ-1708332271176-0.0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d3a6b4d318971-SIN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/completions \"200 OK\"\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': '讲一个关于中国足球的笑话'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E3BB0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248FFEBA240> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248FFF75060>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 08:44:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'890'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'109'), (b'x-ratelimit-remaining-tokens', b'39974'), (b'x-ratelimit-reset-requests', b'10h51m50.405s'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_75e88ecc930d3a2a005f3e2b0568e806'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d3a7b48c63fe0-SIN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='为什么中国足球队总是输球？\\n\\n因为他们每次比赛都把球踢到了长城上！')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"讲一个关于中国足球的笑话\")\n",
    "# llm.invoke(\"讲一个关于中国足球的笑话\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '\\n这是一本历史穿越小说的大纲，具体情节如下：\\n\\n标题：《代码穿越：程序员的宋朝之旅》\\n\\n第一章：意外穿越\\n\\n在一个繁忙的都市街道上，年轻的程序员李明正在匆忙赶往公司。他专注地盯着手机屏幕，完全没有注意到来往的车辆。突然，一辆疾驰而来的卡车冲向了他……\\n\\n李明醒来时，发现自己躺在一个陌生的地方。周围是一片宁静的田野，远处是青山起伏。他惊讶地发现自己已经不在现代城市，而是来到了一片古老的土地上。\\n\\n第二章：古代生活\\n\\n李明意识到自己穿越到了宋朝。他被一个善良的农夫收留，成为了他的儿子。在这个陌生的时代，他学会了耕种、养蚕，体验了古代人的朴素生活。\\n\\n然而，李明心中仍然燃烧着对知识的渴望。他努力自学，夜以继日地钻研经书。他明白，只有通过知识才能改变自己的命运。\\n\\n第三章：求学之路\\n\\n李明决定改变自己的命运。他四处寻找机会，终于得到了一位书香门第的子弟的指点，开始正式求学。\\n\\n在一位隐居山林的老儒家门下，李明接受了艰苦的修行。他学习经史子集，探讨哲理，汲取古人智慧。\\n\\n李明并不满足于只是满足于现状。他立志要通过科举考试，成为一名著名的官员，改变这个时代的命运。\\n\\n第四章：科举之路\\n\\n李明凭借着坚韧的意志和扎实的学识，终于考上了进士。他的名字在科举榜上熠熠生辉，成为了众人瞩目的焦点。\\n\\n然而，进士并不是终点，而是新的起点。李明历经艰难险阻，一步步晋升，最终成为了一位声名显赫的官员。\\n\\n他用自己的才华和智慧，推动着社会的进步，改善着人民的生活。他成为了一个传奇，一个穿越时空的程序员，在古代的世界书写着属于自己的传奇。\\n\\n第五章：回归现代\\n\\n经过岁月的洗礼，李明终于在古代建立了自己的功业。然而，他对现代社会的思念却愈发强烈。\\n\\n在一次偶然的机会下，李明得知了返回现代的方法。他毅然决然地踏上了回归的旅程。\\n\\n当他再次回到了现代的都市，一切都已经不同了。然而，他带着古代的智慧和胸怀，继续着自己的生活，为现代社会的进步贡献着自己的力量。\\n\\n结局：李明虽然回到了现代，但他心中始终怀揣着对古代的回忆和敬意。他用自己的双手书写着跨越时空的传奇，成为了一个永恒的传奇人物。\\n'}, {'role': 'user', 'content': '根据小说大纲将第五章扩展为5000字左右详细情节'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': True, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 20 Feb 2024 07:17:11 GMT'), (b'Content-Type', b'text/event-stream'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'269'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'195'), (b'x-ratelimit-remaining-tokens', b'39321'), (b'x-ratelimit-reset-requests', b'34m10.528s'), (b'x-ratelimit-reset-tokens', b'1.018s'), (b'x-request-id', b'req_6857a8c3028992493c76768f20ea920c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8584f7e7ac433f49-SIN'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第五章：回归现代\n",
      "\n",
      "李明穿越回现代后，他的内心充满了复杂的情绪。一方面，他怀念在古代的那段历程，怀念在宋朝的朴素生活和对知识的追求；另一方面，他也意识到现代社会的发展和变化，感受到了科技带来的便利和进步。然而，他深知自己的责任是在当下的现代社会为之贡献自己的智慧和力量。\n",
      "\n",
      "1.适应现代生活\n",
      "\n",
      "回到现代后，李明发现自己对于现代的一切都有一种新的视角。他重新适应着现代的生活方式，重新熟悉着高楼大厦、繁华街道和快节奏的生活节奏。\n",
      "\n",
      "他租下一间小公寓，开始了自己的新生活。尽管现代的科技和生活方式与古代迥然不同，但他依然保持着对知识的追求和对人文情怀的热爱。\n",
      "\n",
      "2.应用古代智慧\n",
      "\n",
      "在现代社会，李明发现自己对古代的学识和智慧有着独特的应用之处。他将古代的哲学思想和管理经验运用到现代的工作中，取得了意想不到的效果。\n",
      "\n",
      "他以古代的“仁义礼智信”为准则，处理工作中的各种复杂关系；他以古代的“忠孝节义”为信条，对待身边的同事和朋友。这些古代智慧成为了他在现代社会立足的重要法宝。\n",
      "\n",
      "3.影响现代社会\n",
      "\n",
      "随着时间的推移，李明逐渐在现代社会中崭露头角。他凭借着扎实的学识和卓越的能力，成为了公司中不可或缺的人才。\n",
      "\n",
      "他在工作中提出了许多新颖的想法和解决方案，受到了领导和同事的高度赞扬。他用古代的智慧和现代的科技相结合，为公司带来了新的发展机遇。\n",
      "\n",
      "4.寻找古代线索\n",
      "\n",
      "然而，李明的内心始终存在着一种对古代的留恋和思念。在业余时间，他开始寻找古代线索，探究古代文明的奥秘和智慧。\n",
      "\n",
      "他研究古代文献，探讨古代科技的发展，寻找古代留下的种种线索。他希望能够将古代的智慧和现代的科技融合起来，为人类社会带来更大的进步和发展。\n",
      "\n",
      "5.跨越时空的传奇\n",
      "\n",
      "李明成为了一名在现代社会备受尊敬的学者和专家。他不仅在工作中取得了卓越成就，还积极参与公益事业，为社会贡献着自己的力量。\n",
      "\n",
      "他用自己的智慧和胸怀，书写着跨越时空的传奇。他成为了一个连接古代与现代的桥梁，将古代的智慧和现代的科技紧密相连，为人类社会的发展开辟出新的道路。\n",
      "\n",
      "结局\n",
      "\n",
      "李明虽然身处现代社会，但他的内心始终怀揣着对古代的热爱和敬意。他用自己的智慧和努力，为现代社会的进步和发展贡献着自己的力量，成为了一个永恒的传奇人物，留下了不朽的传说。他的故事激励着后人，让人们明白，无论身处何时何地，只要怀揣着对知识和智慧的追求，就能创造出属于自己的传奇。"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_template = '''\n",
    "这是一本历史穿越小说的大纲，具体情节如下：\n",
    "\n",
    "标题：《代码穿越：程序员的宋朝之旅》\n",
    "\n",
    "第一章：意外穿越\n",
    "\n",
    "在一个繁忙的都市街道上，年轻的程序员李明正在匆忙赶往公司。他专注地盯着手机屏幕，完全没有注意到来往的车辆。突然，一辆疾驰而来的卡车冲向了他……\n",
    "\n",
    "李明醒来时，发现自己躺在一个陌生的地方。周围是一片宁静的田野，远处是青山起伏。他惊讶地发现自己已经不在现代城市，而是来到了一片古老的土地上。\n",
    "\n",
    "第二章：古代生活\n",
    "\n",
    "李明意识到自己穿越到了宋朝。他被一个善良的农夫收留，成为了他的儿子。在这个陌生的时代，他学会了耕种、养蚕，体验了古代人的朴素生活。\n",
    "\n",
    "然而，李明心中仍然燃烧着对知识的渴望。他努力自学，夜以继日地钻研经书。他明白，只有通过知识才能改变自己的命运。\n",
    "\n",
    "第三章：求学之路\n",
    "\n",
    "李明决定改变自己的命运。他四处寻找机会，终于得到了一位书香门第的子弟的指点，开始正式求学。\n",
    "\n",
    "在一位隐居山林的老儒家门下，李明接受了艰苦的修行。他学习经史子集，探讨哲理，汲取古人智慧。\n",
    "\n",
    "李明并不满足于只是满足于现状。他立志要通过科举考试，成为一名著名的官员，改变这个时代的命运。\n",
    "\n",
    "第四章：科举之路\n",
    "\n",
    "李明凭借着坚韧的意志和扎实的学识，终于考上了进士。他的名字在科举榜上熠熠生辉，成为了众人瞩目的焦点。\n",
    "\n",
    "然而，进士并不是终点，而是新的起点。李明历经艰难险阻，一步步晋升，最终成为了一位声名显赫的官员。\n",
    "\n",
    "他用自己的才华和智慧，推动着社会的进步，改善着人民的生活。他成为了一个传奇，一个穿越时空的程序员，在古代的世界书写着属于自己的传奇。\n",
    "\n",
    "第五章：回归现代\n",
    "\n",
    "经过岁月的洗礼，李明终于在古代建立了自己的功业。然而，他对现代社会的思念却愈发强烈。\n",
    "\n",
    "在一次偶然的机会下，李明得知了返回现代的方法。他毅然决然地踏上了回归的旅程。\n",
    "\n",
    "当他再次回到了现代的都市，一切都已经不同了。然而，他带着古代的智慧和胸怀，继续着自己的生活，为现代社会的进步贡献着自己的力量。\n",
    "\n",
    "结局：李明虽然回到了现代，但他心中始终怀揣着对古代的回忆和敬意。他用自己的双手书写着跨越时空的传奇，成为了一个永恒的传奇人物。\n",
    "'''\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# 用户聊天模板就是简单的用户聊天内容\n",
    "human_template=\"{input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = chat_prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"input\": \"根据小说大纲将第五章扩展为5000字左右详细情节\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': '给我讲一个关于中国足球的笑话'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248991E1870>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248FFEBA240> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002489B4D96F0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 08:59:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'843'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'110'), (b'x-ratelimit-remaining-tokens', b'39972'), (b'x-ratelimit-reset-requests', b'10h43m43.662s'), (b'x-ratelimit-reset-tokens', b'42ms'), (b'x-request-id', b'req_66c9276665fdbc8a7819a9f3f8d196d6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.4gUkZOlkRJNknKuh0LngVZFaI_p6LCfIkhuc1IP5OM-1708333191-1.0-AflZ1WqyjwrK+JIVTOwwPOU/KTC68Ik0iIqj5pB7iZ4iCxjfa9b3MCtnJURBP0CH7FnUAzXVqBvwJvimIc0MA0E=; path=/; expires=Mon, 19-Feb-24 09:29:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d50e969105f5e-SIN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='为什么中国足球队总是输球？\\n因为他们总是在踢“中国队”。')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"joke\"],\n",
    "    template=\"给我讲一个关于{joke}的笑话\",\n",
    ")\n",
    "prompt = prompt_template.format(joke='中国足球')\n",
    "llm.invoke(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': '给我讲一个关于中国足球的笑话'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000248990F6FE0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000248FFEBA240> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002489B46BAF0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 19 Feb 2024 09:06:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0125'), (b'openai-organization', b'user-afy5yzv93geh2cq6ef0fp6be'), (b'openai-processing-ms', b'981'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'110'), (b'x-ratelimit-remaining-tokens', b'39972'), (b'x-ratelimit-reset-requests', b'10h44m47.258s'), (b'x-ratelimit-reset-tokens', b'42ms'), (b'x-request-id', b'req_07df55581ec2d164ea3d7b5c6bb08ae9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=g0H1oWHN8jQ0tWgaVBar_stuxYUM8X4IhP2TqxSXrNw-1708333560-1.0-AZUSt9jod0/WIhj9d3ZLvACqkRg8SzUd1XoAG54ybfJHtYpMP/UZHV52jtQEJFAk14gYulcLwzfthCeO3lls2G0=; path=/; expires=Mon, 19-Feb-24 09:36:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=SEV__n2eLSnW5TLJXA6QmwMzDcaG5EJ2qdM4VMEscDQ-1708333560425-0.0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'857d59e87b3309f3-LAS'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'joke': '中国足球', 'text': '为什么中国足球队总是输球？\\n\\n因为他们老是在找“球”不着！😂😂😂'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"中国足球\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PydanticOutputParser' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 19\u001B[0m\n\u001B[0;32m     15\u001B[0m new_parser \u001B[38;5;241m=\u001B[39m OutputFixingParser\u001B[38;5;241m.\u001B[39mfrom_llm(parser\u001B[38;5;241m=\u001B[39mparser, llm\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m     17\u001B[0m misformatted \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTom Hanks\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilm_names\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: [\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mForrest Gump\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m]}\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnew_parser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmisformatted\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'PydanticOutputParser' object is not callable"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "\n",
    "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n",
    "\n",
    "new_parser.parser(misformatted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}